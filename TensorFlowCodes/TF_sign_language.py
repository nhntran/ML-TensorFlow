
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-

### Learning TensorFlow with Laurence Moroney, Google Brain on Coursera
### /Users/trannguyen/TranData/WORK/BioinformaticsSpecialization_Tran_2019/\
###/MachineLearning/TensorFlow/TensorFlowCodes

# The dataset is displayed in .csv format.
# Each row is an image. Each row has 784 columns of pixel from 1 to 784

#Download the data
#https://www.kaggle.com/datamunge/sign-language-mnist/home 
#Signing in is needed on kaggle

import os
import csv
import numpy as np
import h5py
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
import keras_preprocessing
from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator


class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('acc')>0.99):
            print("\nReached 99% accuracy so cancelling training!")
            self.model.stop_training=True

def process_csv(filename):

    with open (filename) as myf:
        csv_reader = csv.reader(myf, delimiter = ',')
        first_line = True
        imgs = []
        lbs = []
        for row in csv_reader:
            if first_line:
                #print("Ignoring first line")
                first_line = False
            else:
                lbs.append(row[0])
                img_pixels = row[1:785]
                img_pixels_array = np.array_split(img_pixels, 28)
                imgs.append(img_pixels_array)
        images = np.array(imgs).astype('float')
        labels = np.array(lbs).astype('float')
        #print(len(images))
        #print(len(labels))
    return images, labels

def process_data(local_path1, train_file, test_file):
    
    ### open and process cvs file
    train_images, train_labels = process_csv(local_path1 + train_file)
    test_images, test_labels = process_csv(local_path1 + test_file)

    print(train_images.shape)
    print(train_labels.shape)
    print(test_images.shape)
    print(test_labels.shape)

    #transformation
    train_images = np.expand_dims(train_images, axis = 3)
    test_images = np.expand_dims(test_images, axis = 3)

    train_datagen = ImageDataGenerator(rescale = 1/255,
                                        rotation_range = 40,
                                        width_shift_range = 0.2,
                                        height_shift_range = 0.2,
                                        shear_range = 0.2,
                                        zoom_range = 0.2,
                                        horizontal_flip = True,
                                        fill_mode = 'nearest')
                    #fill empty pixel generated by transformation  

    validation_datagen = ImageDataGenerator(rescale = 1/255)

    train_gen = train_datagen.flow(
        train_images, train_labels, batch_size = 32
        )

    validation_gen = validation_datagen.flow(
        test_images, test_labels, batch_size = 32
        )

    return train_gen, validation_gen, len(train_images), len(test_images)

def building_model(train_gen, validation_gen, len_train, len_test):
    callbacks = myCallback()

    model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    #tf.keras.layers.Dropout(0.5),
    # 128 neuron hidden layer
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(26, activation='softmax')
    ])

    model.summary()
    model.compile(optimizer = 'adam',
                    loss = 'sparse_categorical_crossentropy',
                    metrics = ['acc'])
    

    history = model.fit_generator(
        train_gen,
        steps_per_epoch = len_train/32,
        epochs = 15, verbose = 2,
        validation_data = validation_gen,
        validation_steps = len_test,
        callbacks = [callbacks]
        )
    #verbose=2: Note the values per epoch (loss, accuracy, 
    #validation loss, validation accuracy)
    ## retrieve values
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    #evaluate the model
    model_evaluation(acc, val_acc, loss, val_loss)
    ## save model
    model.save('TF_rock_paper_scissors.h5') #after training, saving the model into the .h5 file

    return model

def model_evaluation(acc, val_acc, loss, val_loss):
    epochs = range(len(acc)) #get number of epochs

    #plot training and validation accuracy and loss
    plt.plot(epochs,acc, 'r', "Training Accuracy")
    plt.plot(epochs,val_acc, 'b')
    plt.title("Training and validation accuracy")

    plt.plot(epochs,loss)
    plt.plot(epochs,val_loss)
    plt.title("Training and validation loss")

def prediction_rock_paper_scissors(local_path2, model):
    prediction_dir = os.path.join(local_path2)
    prediction_names = os.listdir(prediction_dir)

    for fn in prediction_names:
     
      img = image.load_img(local_path2+fn, target_size=(150, 150))
      x = image.img_to_array(img)
      x = np.expand_dims(x, axis=0)

      images = np.vstack([x])
      classes = model.predict(images, batch_size=10)
      print(fn)
      print(classes)


########################################################################
# The main() function
def main():
    
    #print(tf.__version__)
    local_path1 = 'data/sign-language-data/'
    #local_path2 = 'data/rock-paper-scissors-data/prediction/'
    train_file ='sign_mnist_train.csv'
    test_file ='sign_mnist_test.csv'

    train_gen, validation_gen, len_train, len_test = process_data(local_path1, 
                                train_file, test_file)
    model = building_model(train_gen, validation_gen, 
                    len_train, len_test)
    #prediction_rock_paper_scissors(local_path2, model)

    # Second time running: Loading the model again
    # new_model = tf.keras.models.load_model('TF_Dogs_Cats_TransferLearning0.h5')
    # prediction_cat_dog(local_path2, new_model)

#######################################################################
# Standard boilerplate to call the main() function to begin
# the program.
if __name__ == '__main__':
  main()

